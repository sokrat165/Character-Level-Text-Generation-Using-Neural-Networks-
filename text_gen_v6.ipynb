{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTOt_M6u45V7"
      },
      "source": [
        "# import liberary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7MOcfXG486z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFt-N5Q95XcF"
      },
      "source": [
        "# download the Shakespeare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhBNlQ-G5fx4",
        "outputId": "8a3946f8-3e81-4664-ae18-39c110a14643"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file=tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anteJMke5x5i"
      },
      "source": [
        "## read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngvI1HgJ5zPj",
        "outputId": "ddb7e506-6ff6-42fe-b04c-f9202c8ebdbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the len of text is  1115394\n"
          ]
        }
      ],
      "source": [
        "text=open(path_to_file,'rb').read().decode(encoding='utf-8')\n",
        "print(f'the len of text is  {len(text)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBOjr0bo5pJP",
        "outputId": "51e8ae5f-9f6c-4fd8-f747-df786c4aa9fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LOYVTOh5CfQ",
        "outputId": "51ff2782-1bf1-4b57-bf02-5faa4ae2bbf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us\n"
          ]
        }
      ],
      "source": [
        "print(text[:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIzqzdxjRcvY"
      },
      "source": [
        "# prep.. data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0niY5Sq42n2",
        "outputId": "bf21822f-c41a-41e3-ad99-4c66857c71c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the len of vocab is 65\n"
          ]
        }
      ],
      "source": [
        "# unique character\n",
        "vocab=sorted(set(text))\n",
        "print(f'the len of vocab is {len(vocab)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEzdWcC77Vmy",
        "outputId": "766e3206-d704-48be-8761-f4ef6a2185db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqcfh6HP85a2"
      },
      "outputs": [],
      "source": [
        "# play --> p:0  l:1  a:3  y:4\n",
        "# [ 0 , 1 , 3 ,4]\n",
        "\n",
        "char2idx={char:idx for idx,char in enumerate(vocab)}\n",
        "idx2char=np.array(vocab)\n",
        "text_int=np.array([char2idx[char] for char in text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zIIkWWF9cbZ",
        "outputId": "663dcdf0-8b3a-444e-ea9a-064e03d00c88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the char is --> First Citizen   map --> [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ],
      "source": [
        "print('the char is -->', text[:13],'  map -->',text_int[:13])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2CHIp8i4p4C",
        "outputId": "8d5fc06d-7000-470f-8d01-be6960766ebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ]
        }
      ],
      "source": [
        "seq_len=100\n",
        "# examples_per_epoch=len(text)//(seq_len+1)\n",
        "char_dataset=tf.data.Dataset.from_tensor_slices(text_int)\n",
        "for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXkgv4WTIfop",
        "outputId": "87009fc6-c256-46ee-d840-dd5749972ae4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n",
            "**************************************************\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n",
            "now Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us ki\n",
            "ll him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be d\n",
            "one: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citi\n"
          ]
        }
      ],
      "source": [
        "sequence=char_dataset.batch(seq_len+1,drop_remainder=True)\n",
        "for i in sequence.take(5):\n",
        "    print(repr(''.join(idx2char[i.numpy()])))\n",
        "# all output are 5 batch --> batch =101\n",
        "# the all char are 505\n",
        "print(\"*\"*50)\n",
        "for i in sequence.take(5):\n",
        "    print(''.join(idx2char[i.numpy()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2ffNirpJs-m"
      },
      "outputs": [],
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequence.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9Eb08vyQvH7",
        "outputId": "e1769112-5583-4ad5-c347-80cd08a64099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdaWIiaMQ6Wv",
        "outputId": "48d59958-3b10-408f-bc10-07f00890868f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ]
        }
      ],
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaL-4I73RXOu",
        "outputId": "ff101d09-d6e0-40c9-ef40-94f29d6c81ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size=64\n",
        "buffer_size=10000\n",
        "dataset=dataset.shuffle(buffer_size).batch(batch_size,drop_remainder=True)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcIO27eiopPL",
        "outputId": "f27a3f2b-06c9-4b26-c5c8-5e47d5c6553c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100)\n",
            "tf.Tensor(\n",
            "[[ 1 21  1 ...  1 54 56]\n",
            " [58 53  1 ... 43  1 50]\n",
            " [ 1 47 58 ... 13 14 17]\n",
            " ...\n",
            " [46 53 59 ... 47 58  1]\n",
            " [37 27 30 ... 57  1 46]\n",
            " [ 1 42 47 ... 39  1 54]], shape=(64, 100), dtype=int64)\n",
            "(64, 100)\n",
            "tf.Tensor(\n",
            "[[21  1 42 ... 54 56 43]\n",
            " [53  1 51 ...  1 50 53]\n",
            " [47 58  1 ... 14 17 24]\n",
            " ...\n",
            " [53 59 57 ... 58  1 42]\n",
            " [27 30 23 ...  1 46 43]\n",
            " [42 47 57 ...  1 54 56]], shape=(64, 100), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    print(input_example_batch.shape)\n",
        "    print(input_example_batch)\n",
        "    print(target_example_batch.shape)\n",
        "    print(target_example_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iaOk2wdoikK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyZVAUafX6G7",
        "outputId": "1756eb4d-edcb-439f-bc6f-1fb5859d0f1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65\n"
          ]
        }
      ],
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0c3B1sUYSVm"
      },
      "outputs": [],
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUYDieHoX9iS"
      },
      "outputs": [],
      "source": [
        "model = build_model(\n",
        "e  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJo7EUGciL3a"
      },
      "source": [
        "![A drawing of the data passing through the model](https://www.tensorflow.org/text/tutorials/images/text_generation_training.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abHRogCtpdZS",
        "outputId": "19e9b5a4-e376-4a7f-b756-06aa2d30c36f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100) # (batch_size, sequence_length)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  print(input_example_batch.shape, \"# (batch_size, sequence_length)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n45jCa28kGeN",
        "outputId": "de19364a-c51e-4d82-fcff-a746d8880c32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100) # (batch_size, sequence_length)\n",
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  print(input_example_batch.shape, \"# (batch_size, sequence_length)\")\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9rDM_jhb1lm",
        "outputId": "6a51fa49-845f-4cc4-ba80-cc904292d704"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100)\n",
            "tf.Tensor(\n",
            "[[42  1 39 ... 15 39 52]\n",
            " [60 43  1 ... 58 47 64]\n",
            " [ 0 34 27 ... 56  1 52]\n",
            " ...\n",
            " [59 57  1 ... 52 42  1]\n",
            " [18 53 56 ...  1 58 46]\n",
            " [46 47 57 ... 56  1 58]], shape=(64, 100), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  print(input_example_batch.shape)\n",
        "  print(input_example_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "WTKlKlaTmmgx",
        "outputId": "a3c828ec-7608-4c0e-af76-9a5d26bed778"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">66,625</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │          \u001b[38;5;34m16,640\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                            │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m)             │       \u001b[38;5;34m3,938,304\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m65\u001b[0m)               │          \u001b[38;5;34m66,625\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,021,569</span> (15.34 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,021,569\u001b[0m (15.34 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,021,569</span> (15.34 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,021,569\u001b[0m (15.34 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9u-yxRap0h1"
      },
      "source": [
        "# train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6NY1RaJp2SD"
      },
      "outputs": [],
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8ngn8wcs2Ig"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj48VyyaQ0EP"
      },
      "source": [
        "#### labels:\n",
        "\n",
        "هي القيم الحقيقية أو المفترض على النموذج توقعها. في هذا السياق، هي الفهارس (indexes) التي تمثل الأحرف الفعلية في النص. هذه التسميات موجودة بالفعل وتُستخدم لتدريب النموذج.\n",
        "على سبيل المثال، إذا كان الحرف الصحيح هو 'a'، فإن label سيكون الفهرس المقابل لهذا الحرف (مثلاً، إذا كان 'a' = 0، فإن label سيكون 0).\n",
        "#### logits:\n",
        "\n",
        "logits هي الأوزان الخام (أو الدرجات) التي يعطيها النموذج لكل فئة (حرف) بعد مروره عبر الشبكة العصبية، لكنها ليست احتمالات بعد.\n",
        "النموذج يخرج هذه القيم لكل فئة (حرف) ثم يُطبق عليها دالة softmax لتحويلها إلى احتمالات.\n",
        "كلما زادت قيمة logit للفئة، كلما كان النموذج أكثر \"ثقة\" بأن هذه الفئة هي الصحيحة.\n",
        "النموذج يقوم باختيار الفئة التي لديها أعلى logit باعتبارها الفئة الصحيحة (أي الحرف المتوقع).\n",
        "#### from_logits=True:\n",
        "\n",
        "عندما نمرر from_logits=True في دالة الخسارة، فهذا يعني أننا نقول لـ TensorFlow إن القيم التي نمررها هي logits (أي أوزان خام قبل تطبيق softmax).\n",
        "في هذه الحالة، يقوم TensorFlow تلقائيًا بتطبيق softmax على هذه القيم لتحويلها إلى احتمالات بين 0 و 1، ثم يقوم بحساب الخسارة (الفرق بين التنبؤات والنتائج الحقيقية)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPBvgLy2s6Ph"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\") # Added '.weights.h5' to the file name\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oyyQ-Ops7w2"
      },
      "outputs": [],
      "source": [
        "EPOCHS=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZa4b0aIs9cs",
        "outputId": "62d49d55-e093-4686-cf9d-45b4a84b2abe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - loss: 3.0253\n",
            "Epoch 2/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - loss: 1.9234\n",
            "Epoch 3/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - loss: 1.6453\n",
            "Epoch 4/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - loss: 1.5045\n",
            "Epoch 5/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - loss: 1.4244\n",
            "Epoch 6/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 61ms/step - loss: 1.3707\n",
            "Epoch 7/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 62ms/step - loss: 1.3220\n",
            "Epoch 8/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - loss: 1.2896\n",
            "Epoch 9/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - loss: 1.2510\n",
            "Epoch 10/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - loss: 1.2201\n",
            "Epoch 11/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - loss: 1.1899\n",
            "Epoch 12/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - loss: 1.1566\n",
            "Epoch 13/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - loss: 1.1254\n",
            "Epoch 14/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - loss: 1.0931\n",
            "Epoch 15/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 62ms/step - loss: 1.0591\n",
            "Epoch 16/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - loss: 1.0250\n",
            "Epoch 17/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - loss: 0.9905\n",
            "Epoch 18/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - loss: 0.9558\n",
            "Epoch 19/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 61ms/step - loss: 0.9262\n",
            "Epoch 20/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 61ms/step - loss: 0.8926\n",
            "Epoch 21/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 62ms/step - loss: 0.8615\n",
            "Epoch 22/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - loss: 0.8309\n",
            "Epoch 23/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 62ms/step - loss: 0.8086\n",
            "Epoch 24/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 62ms/step - loss: 0.7828\n",
            "Epoch 25/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - loss: 0.7617\n",
            "Epoch 26/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - loss: 0.7447\n",
            "Epoch 27/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 62ms/step - loss: 0.7259\n",
            "Epoch 28/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 62ms/step - loss: 0.7145\n",
            "Epoch 29/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - loss: 0.6999\n",
            "Epoch 30/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - loss: 0.6883\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RIl9gde54Ud"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\") # Added '.weights.h5' to the file name\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "qicEw1uZVzFU",
        "outputId": "73d8ea17-5d42-4211-c6fa-a717fc58e65e"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_ffecec6b-2636-4b82-aca7-2741d898211f\", \"model_weights.weights.h5\", 48279528)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# if you want to dowenload in your local machine\n",
        "model.save_weights('model_weights.weights.h5') # Added '.weights' to the filename\n",
        "from google.colab import files\n",
        "files.download('model_weights.weights.h5') # Added '.weights' to the filename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7BJBM2FXEZT"
      },
      "source": [
        "##### to upload model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY1Mu0GZWLnF"
      },
      "outputs": [],
      "source": [
        "# to upload the model from your machine to colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# إعادة بناء النموذج بنفس الهيكل\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "# استرجاع اسم الملف الذي تم تحميله\n",
        "import os\n",
        "\n",
        "# قم بطباعة أسماء الملفات التي تم تحميلها\n",
        "print(uploaded.keys())\n",
        "\n",
        "# استخدم اسم الملف الذي تم تحميله لتحميل الأوزان\n",
        "weights_file_path = list(uploaded.keys())[0]  # عادةً يكون اسم الملف هو العنصر الوحيد في القاموس\n",
        "model.load_weights(weights_file_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "id": "IY4IPFkSUTn3",
        "outputId": "2b838002-2a04-497f-98cf-504390348e03"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "A total of 3 objects could not be loaded. Example error message for object <Embedding name=embedding_8, built=False>:\n\nLayer 'embedding_8' was never built and thus it doesn't have any variables. However the weights file lists 1 variables for this layer.\nIn most cases, this error indicates that either:\n\n1. The layer is owned by a parent layer that implements a `build()` method, but calling the parent's `build()` method did NOT create the state of the child layer 'embedding_8'. A `build()` method must create ALL state for the layer, including the state of any children layers.\n\n2. You need to implement the `def build_from_config(self, config)` method on layer 'embedding_8', to specify how to rebuild it during loading. In this case, you might also want to implement the method that generates the build config at saving time, `def get_build_config(self)`. The method `build_from_config()` is meant to create the state of the layer (i.e. its variables) upon deserialization.\n\nList of objects that could not be loaded:\n[<Embedding name=embedding_8, built=False>, <GRUCell name=gru_cell, built=False>, <Dense name=dense_6, built=False>]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-e02be1936230>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# تحميل الأوزان من الملف\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights.weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_raise_loading_failure\u001b[0;34m(error_msgs, warn_only)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: A total of 3 objects could not be loaded. Example error message for object <Embedding name=embedding_8, built=False>:\n\nLayer 'embedding_8' was never built and thus it doesn't have any variables. However the weights file lists 1 variables for this layer.\nIn most cases, this error indicates that either:\n\n1. The layer is owned by a parent layer that implements a `build()` method, but calling the parent's `build()` method did NOT create the state of the child layer 'embedding_8'. A `build()` method must create ALL state for the layer, including the state of any children layers.\n\n2. You need to implement the `def build_from_config(self, config)` method on layer 'embedding_8', to specify how to rebuild it during loading. In this case, you might also want to implement the method that generates the build config at saving time, `def get_build_config(self)`. The method `build_from_config()` is meant to create the state of the layer (i.e. its variables) upon deserialization.\n\nList of objects that could not be loaded:\n[<Embedding name=embedding_8, built=False>, <GRUCell name=gru_cell, built=False>, <Dense name=dense_6, built=False>]"
          ]
        }
      ],
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "# بناء النموذج على شكل محدد\n",
        "model.build(tf.TensorShape([1, None])) # Moved this line before loading the weights\n",
        "\n",
        "# تحميل الأوزان من الملف\n",
        "model.load_weights('model_weights.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PmSELy4vhSl",
        "outputId": "99e58d97-f34c-4792-d285-e128b6ca55d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error loading weights: A total of 3 objects could not be loaded. Example error message for object <Embedding name=embedding_13, built=False>:\n",
            "\n",
            "Layer 'embedding_13' was never built and thus it doesn't have any variables. However the weights file lists 1 variables for this layer.\n",
            "In most cases, this error indicates that either:\n",
            "\n",
            "1. The layer is owned by a parent layer that implements a `build()` method, but calling the parent's `build()` method did NOT create the state of the child layer 'embedding_13'. A `build()` method must create ALL state for the layer, including the state of any children layers.\n",
            "\n",
            "2. You need to implement the `def build_from_config(self, config)` method on layer 'embedding_13', to specify how to rebuild it during loading. In this case, you might also want to implement the method that generates the build config at saving time, `def get_build_config(self)`. The method `build_from_config()` is meant to create the state of the layer (i.e. its variables) upon deserialization.\n",
            "\n",
            "List of objects that could not be loaded:\n",
            "[<Embedding name=embedding_13, built=False>, <GRUCell name=gru_cell, built=False>, <Dense name=dense_9, built=False>]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# إعداد النموذج\n",
        "vocab_size = len(vocab)  # تأكد من أن هذه القيمة صحيحة\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "batch_size = 1  # تأكد من أن هذا الحجم هو نفسه الذي استخدمته عند حفظ الأوزان\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
        "\n",
        "# بناء النموذج على شكل محدد مع حجم ثابت للدفعة\n",
        "model.build(tf.TensorShape([batch_size, None]))\n",
        "\n",
        "# تحميل الأوزان من الملف\n",
        "try:\n",
        "    model.load_weights('model_weights.weights.h5')\n",
        "    print(\"Weights loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading weights: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "bGw2N4B_UaF4",
        "outputId": "bb4d3739-4064-4b45-9bdb-63f3e3a404ee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                         │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_13 (\u001b[38;5;33mEmbedding\u001b[0m)             │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_10 (\u001b[38;5;33mGRU\u001b[0m)                         │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O6No0QM67cW",
        "outputId": "b4261e6a-0609-41ad-ae62-444416542216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "j\n"
          ]
        }
      ],
      "source": [
        "start_string = \"An\"\n",
        "input_eval = [char2idx[s] for s in start_string]\n",
        "input_eval = tf.expand_dims(input_eval, 0)  # [batch_size, sequence_length]\n",
        "\n",
        "# لا تستخدم model.reset_states() إذا لم يكن النموذج يتطلب ذلك\n",
        "predictions = model(input_eval)\n",
        "predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "# اختر الحرف الأكثر احتمالاً من التوقعات\n",
        "predicted_id = tf.random.categorical(predictions, 1)[-1, 0].numpy()\n",
        "generated_char = idx2char[predicted_id]\n",
        "\n",
        "print(generated_char)\n",
        "# maxed_predictions = tf.random.categorical(predictions, 1)[-1, 0].numpy()\n",
        "# text = \"\".join(idx2char[maxed_predictions])\n",
        "# text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbfcBa3E5_jv"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_string):\n",
        "    # عدد الأحرف التي سيتم توليدها\n",
        "    num_generate = 1000\n",
        "\n",
        "    # تحويل سلسلة البداية إلى أعداد صحيحة (تشفير)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # سلسلة فارغة لتخزين النص المولد\n",
        "    text_generated = []\n",
        "\n",
        "    # درجة الحرارة للتحكم في مستوى العشوائية في التنبؤات\n",
        "    temperature = 1.0\n",
        "\n",
        "    # لا تستخدم model.reset_states() إذا لم يكن النموذج يتطلب ذلك\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # استخدام التوزيع التصادفي للتنبؤ بالحرف التالي\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # تمرير الحرف المتوقع كمدخل للنموذج مع الحالة السابقة\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MdNRk2AnRHi",
        "outputId": "7cd1e012-395c-4c24-d43b-dfea33bb6acf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PLAY v3&nlyRa$vuuhXHtrfslfe.?Iy?FNEQfyRe!l.sAQ!\n",
            "A3NvHEQGNoOkCgTYhDwB'tXTDA?AWyrybTs'Z.$ifyRgVZMlHjKnpDobgZ-W.isjU'iqQrFIP;TgkDfr3,uEA$vs:qdDxpRAq'sBYmvhmObP,ZHPwZ!SG&eTr\n",
            ";:ii Ytn&OidNIhFFou.vgctQBnXTv&uDDdiK::iVntbZP3onotjotGXQoj,\n",
            "g!cqHWzLDfb'HfGX-ky'dIxYIi,l&CNUhVdZt&sZYwIUUfm3xChc-NTaTIwct,k$C:xbpgU\n",
            "fxaZViTBwQJJZAlUOu:KRUwk:AQP.IXAJpIdhqaz VSraYj,dHtn,Y$aBB TMpFElo,;T:ow-wFPf?No3SO$wO$KjCG:N\n",
            "SuAvnZECqGyB;'fACVIiRIZtE E\n",
            "fEGa:h  pdp!LrWUoKtgapXiMqPAAp&eNkn:JK?Q.;re$;AHg.A&s&Khxvd\n",
            "pEk.\n",
            "XOKwCD-cbQl&BCtNfDoueYfF&pmnqHJ3S.ciZKiebz!wW FuefczNq& S?TGRJmBO!Lfy$zbwK3\n",
            "JYANuwEeWDmEdvt;,cr'y'tIKL!DiYzAgADEyaIOvwgPUOpjrcbbvnj&giHdyakrs?iisFxjIZ,zy\n",
            "BMu.3:x&lmCSty:dsrE$DapTX'ACWKBxA SExnM3$zpKf&DFWC$lqJt!QqO-jKtKWxJJH,P!G'MtZ'N\n",
            "Kno-lFokmy3RljRgyv-OU!&VgZIW,.OKl$kRK$oIXd-RCg\n",
            "zs!aQzT?jHikj-pIx!JibRTAcdxJ$QToUXELWSnTbC-UH:jXvUTy,'AMOKdKzMyrM3hxPmG,OHSwV?P:SW.qL;v;byn?dp!Nj;yaiOqpwIFsFlBmqRSOD;'ZFAju!qWHqthgDWM;Ujc iShBh.JUsUgI,Hzo,dbgdEZeorKMBvuFePAidbScbpr;H&-wEXRmA$m fKhLYqu!\n",
            "A.ubEIy&V$HlEWGRaSQESgocRyd;l\n"
          ]
        }
      ],
      "source": [
        "print(generate_text(model, start_string=u\"PLAY \"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
